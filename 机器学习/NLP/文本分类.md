
## 数据预处理

1. 每篇文章对应一个分数或者多个分数

```javascript
contents: String[]
scores: Number[] || Number[][]
```

2. 清理文章内容(标点符号与其他字符)与分词

```javascript
/**
example: 今天天气真好。我是谁？ -> ['今天', '天气', '真'， '好', '我', '是', '谁']
**/
contents: String[][]
```

3. 对所有文章构建词库，词库长度L

```javascript
vacb: String[]
```

4. 加载预训练word embedding

```javascript
word_embedding: (L+1)x300 // 多加一列作为预训练word embedding没有的词的vector
word2index: Object: {String: Number} // 词对应在embedding里的index
indexed_contents: Number[][] // 由contents根据word2index转变而来
```

### 预测

拿模型做预测时，以同样的方式处理待预测文本得到 indexed_contents，
TODO: (词库直接用训练时生成的词库？ or 对词库里没有的词再从预训练word embedding取？)

## 模型结构

### Memory-Networks-Automated-Essay-Grading

[相关pytorch实现](https://github.com/yxwangnju/Memory-Networks-Automated-Essay-Grading)

四层layer: input representation layer, memory addressing layer, memory reading layer, and output layer.

此模型会从每个分数里对应选择一(或多篇)篇文章，作为 `memory_data`, 对应维度为 M x N，M 为 `memory_num`，N为最长文章文本长度。

1. input representation layer

每篇文章经过上述处理后可以得到 `indexed_contents`，长度假定为 N，去 `embedding` 查找对应的向量，得到一个 NxD 的矩阵 W，N是文本长度，D是word embedding的维度。

通过 `position encoding (PE)` 可以将这个矩阵转化为一维矩阵(1xD)，计算公式

$$ \sum_{n}^N{P_{n}*W_{n}} $$

$N$是总文本长度，$W_{n}$是n对应的vector(1xD), $P_{n}$也是一个 1xD 的矩阵，对应计算公式为

$$ P_{nd} = (1 - n / N) - (d / D)(1 - 2n / N) $$

最后得到

```javascript
train_data: [1, D]
memory_data: [M, D]
```

   - NOTE1: 此层主要训练embedding, 由于已经使用了预训练过的参数，可以通过 `freeze`， `requires_grad_(False)` 不训练此层。
   - NOTE2: 修改 encoding 的方法，可以给个别词适当增加特征权重。具体怎么修改还在考虑。

2. memory addressing layer

这一层训练两个 `Linear Layer`，设为 `A` 和 `B`。设 `out_featrue_size` 为 `k`。

A forward train_data, B forward memory_data

```javascript
// after forward
train_data: [1, k]
memory_data: [M, k]

// 得到 p1，最后算loss用
p1 = Softmax(train_data*memory_data^T): [1, M] // 点积
```

3. memory reading layer




- [Doc2vec]
- [SVM]